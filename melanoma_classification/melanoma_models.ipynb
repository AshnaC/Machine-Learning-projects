{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import gc\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from albumentations import (Compose, RandomResizedCrop, HorizontalFlip, \n",
    "                            VerticalFlip, HueSaturationValue,RandomBrightnessContrast,\n",
    "                            Resize, Normalize )\n",
    "from albumentations.pytorch import ToTensorV2, ToTensor\n",
    "\n",
    "from torchvision.models import resnet34, resnet50\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from sklearn.model_selection import GroupKFold,StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def set_seed(seed = 1234):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device available now:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "baseDir ='../input/siim-isic-melanoma-classification'\n",
    "extDir ='../input/melanoma-external-malignant-256'\n",
    "train_df = pd.read_csv(baseDir + '/train.csv')\n",
    "test_df = pd.read_csv(baseDir + '/test.csv')\n",
    "train_ext = pd.read_csv(extDir + '/train_concat.csv')\n",
    "train_df.drop(['diagnosis','benign_malignant'], inplace=True, axis=1)\n",
    "images_ids = train_df['image_name'].unique()\n",
    "new_data = train_ext[~train_ext['image_name'].isin(images_ids)]\n",
    "# train_df['image_path'] = baseDir+'/jpeg/train/'+ train_df['image_name']+'.jpg'\n",
    "# test_df['image_path'] = baseDir+'/jpeg/test/'+ test_df['image_name']+'.jpg'\n",
    "# new_data['image_path'] = extDir+'/train/train/'+ new_data['image_name']+'.jpg'\n",
    "\n",
    "train_df['image_path'] = extDir+'/train/train/'+ train_df['image_name']+'.jpg'\n",
    "test_df['image_path'] = extDir+'/test/test/'+ test_df['image_name']+'.jpg'\n",
    "new_data['image_path'] = extDir+'/train/train/'+ new_data['image_name']+'.jpg'\n",
    "\n",
    "train = pd.concat([train_df,new_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Missing values filled with maximum values\n",
    "train['patient_id'] =train['patient_id'].fillna(\"0\")\n",
    "train['age_approx'] =train['age_approx'].fillna(45)\n",
    "train['sex'] =train['sex'].fillna('male')\n",
    "train['anatom_site_general_challenge'] =train['anatom_site_general_challenge'].fillna('torso')\n",
    "\n",
    "# Imputing for test as well\n",
    "test_df['anatom_site_general_challenge'] =test_df['anatom_site_general_challenge'].fillna('torso')\n",
    "\n",
    "cat_cols =['sex','anatom_site_general_challenge']\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    train[col] = le.fit_transform(train[col])\n",
    "    test_df[col] = le.transform(test_df[col])\n",
    "train.rename(columns={\"anatom_site_general_challenge\":\"anatomy\",\"age_approx\":\"age\" },inplace=True)\n",
    "test_df.rename(columns={\"anatom_site_general_challenge\":\"anatomy\",\"age_approx\":\"age\" },inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_path = train_df.iloc[0]['image_path']\n",
    "org_img = cv2.imread(img_path)\n",
    "# img= ToPILImage()(org_img)\n",
    "img= HueSaturationValue(sat_shift_limit=[0.7, 1.3],hue_shift_limit=[-0.1, 0.1])(image=org_img)['image']\n",
    "fig,axes =plt.subplots(1,2)\n",
    "axes[0].imshow(org_img)\n",
    "axes[1].imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MelanomaDataset(Dataset):\n",
    "    def __init__(self, df,vert_flip, horz_flip, full_transform = True, isTest= False):\n",
    "        self.df = df\n",
    "        self.full_transform = full_transform\n",
    "        self.isTest = isTest\n",
    "        if full_transform:\n",
    "            self.transforms = Compose([\n",
    "                RandomResizedCrop(height= 224,width= 224, scale= (0.4,1)),\n",
    "                HorizontalFlip(p=horz_flip),\n",
    "                VerticalFlip(p = vert_flip),\n",
    "                HueSaturationValue(sat_shift_limit=[0.7, 1.3],hue_shift_limit=[-0.1, 0.1]),\n",
    "                RandomBrightnessContrast(brightness_limit=[0.7, 1.3],contrast_limit= [0.7, 1.3]),\n",
    "                Normalize(),\n",
    "                ToTensor()\n",
    "            ])\n",
    "        else:\n",
    "            self.transforms = Compose([Normalize(),ToTensor()])\n",
    "#             self.transforms = Compose([Resize(height= 1024,width= 1024),Normalize(),ToTensor()])\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df['image_path'][idx]\n",
    "        img_arr = cv2.imread(img_path)\n",
    "        img_aug = self.transforms(image = img_arr)\n",
    "        image = img_aug['image']\n",
    "        csv_data = np.array(train.iloc[idx][['sex', 'age', 'anatomy']].values, dtype = np.float32)\n",
    "        if self.isTest:\n",
    "            return image,csv_data\n",
    "        else:\n",
    "            return (image,csv_data ) , self.df['target'][idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dataset = MelanomaDataset(train[0:100],0.5, 0.5, full_transform = False)\n",
    "# dataloader = DataLoader(dataset,batch_size=3, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sample_csv =None\n",
    "# sample_images = None\n",
    "# sample_labels = None\n",
    "# for (images, csv_data), labels in dataloader:\n",
    "#     sample_csv = csv_data \n",
    "#     sample_images = images\n",
    "#     sample_labels = torch.tensor(labels, dtype=torch.float32)\n",
    "#     print('images',images.shape)\n",
    "#     print('csv_data',csv_data.shape)\n",
    "#     print('label',labels)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Resnet50Netwrok(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = resnet50(pretrained = True)\n",
    "        # Final output size - Probabiltiy if melonoma or not\n",
    "        self.output_size = 1\n",
    "        # Considering csv features age, sex\n",
    "        self.csv_cols = 3\n",
    "        self.csv_network = nn.Sequential(nn.Linear(self.csv_cols, 500), \n",
    "                                         nn.BatchNorm1d(500), \n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Dropout(p=0.2))\n",
    "        self.final_layer = nn.Linear(1500,self.output_size)\n",
    "    \n",
    "    def forward(self, image, csv_data):\n",
    "        image = self.model(image)\n",
    "        #print(\"Shape of image out from RestNet\", image.shape)\n",
    "        csv_nn_data = self.csv_network(csv_data)\n",
    "        #print(\"Shape of csv out from NN\", csv_data.shape)\n",
    "        \n",
    "        # Concat out from csv data and image to pass to final\n",
    "        # Classification layer\n",
    "        data = torch.cat((image,csv_nn_data), dim=1)\n",
    "        print(\"Final concat data shape\", data.shape)\n",
    "        \n",
    "        out = self.final_layer(data)\n",
    "        #print(\"Output shape\", out.shape)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sample_resenet = Resnet50Netwrok()\n",
    "# sample_out = sample_resenet(sample_images,sample_csv )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Criterion example\n",
    "# criterion_example = nn.BCEWithLogitsLoss()\n",
    "# # Unsqueeze(1) from shape=[3] to shape=[3, 1]\n",
    "# loss = criterion_example(sample_out, sample_labels.unsqueeze(1))   \n",
    "# print('Loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -q efficientnet_pytorch             \n",
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class EfficientNetwork(nn.Module):\n",
    "    def __init__(self, modeltype):\n",
    "        super().__init__()\n",
    "        modelName = 'efficientnet-'+ modeltype\n",
    "        self.model = EfficientNet.from_pretrained(modelName)\n",
    "        # Final output size - Probabiltiy if melonoma or not\n",
    "        self.output_size = 1\n",
    "        # Considering csv features age, sex\n",
    "        self.csv_cols = 3\n",
    "        self.csv_network = nn.Sequential(nn.Linear(self.csv_cols, 250), \n",
    "                                         nn.BatchNorm1d(250), \n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Dropout(p=0.2),\n",
    "                                         \n",
    "                                         nn.Linear(250, 250), \n",
    "                                         nn.BatchNorm1d(250), \n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Dropout(p=0.2))\n",
    "        nodel_node_count = 2560\n",
    "        if modeltype == \"b4\":\n",
    "            nodel_node_count = 1792\n",
    "        if modeltype == \"b2\":\n",
    "            nodel_node_count = 1408\n",
    "        if modeltype == \"b6\":\n",
    "            nodel_node_count = 2304\n",
    "            \n",
    "        \n",
    "        final_layer_nodes = nodel_node_count+ 250\n",
    "        self.final_layer = nn.Sequential(nn.Linear(final_layer_nodes,self.output_size ))\n",
    "\n",
    "    def forward(self, images, csv_data):\n",
    "        # Extract image features\n",
    "        images_feature = self.model.extract_features(images)\n",
    "        #print(\"Feature shape from model\", images_feature.shape)\n",
    "        feat_shape = images_feature.shape\n",
    "        (m,n,fil1, fil2) = feat_shape\n",
    "        \n",
    "        #print('Feature count from model', n)\n",
    "        \n",
    "        # Average pooling by (7,7) filters\n",
    "        # image = F.avg_pool2d(image, image.size()[2:]).reshape(-1, 1792)\n",
    "        pooled_imgs = F.avg_pool2d(images_feature, (fil1, fil2)).reshape(m,n)\n",
    "        #print('Pooled image shape', pooled_imgs.shape) \n",
    "   \n",
    "        # Add csv layer\n",
    "        #print('CSV data shape ',csv_data.shape)\n",
    "        csv_nn_data = self.csv_network(csv_data)\n",
    "        \n",
    "        #print('CSV output shape ',csv_nn_data.shape)\n",
    "        # Concat both layers\n",
    "        data = torch.cat((pooled_imgs,csv_nn_data), dim=1)\n",
    "        #print('Concat data shape ', data.shape)\n",
    "        \n",
    "        # Apply final layer\n",
    "        out = self.final_layer(data)\n",
    "        #print('Output shape ', out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sample_effnet = EfficientNetwork(\"b2\")\n",
    "# sample_eff_out = sample_effnet(sample_images,sample_csv )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Criterion example\n",
    "# criterion_example = nn.BCEWithLogitsLoss()\n",
    "# # Unsqueeze(1) from shape=[3] to shape=[3, 1]\n",
    "# loss = criterion_example(sample_eff_out, sample_labels.unsqueeze(1))   \n",
    "# print('Loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# epochs = 15\n",
    "# patience = 3\n",
    "# TTA = 3\n",
    "# num_workers = 8\n",
    "# learning_rate = 0.0005\n",
    "# weight_decay = 0.0\n",
    "# lr_patience = 1            # 1 model not improving until lr is decreasing\n",
    "# lr_factor = 0.4            # by how much the lr is decreasing\n",
    "\n",
    "# batch_size1 = 32\n",
    "# batch_size2 = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getDataLoaoders(train_index,test_index):\n",
    "    train_data = train.iloc[train_index].reset_index(drop= True)\n",
    "    valid_data = train.iloc[test_index].reset_index(drop= True)\n",
    "    #print('valid_data', valid_data['target'])\n",
    "    train_ds= MelanomaDataset(train_data,0.5, 0.5, full_transform = True)\n",
    "    valid_ds= MelanomaDataset(valid_data,0.5, 0.5, full_transform = False)\n",
    "    # train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers =8)\n",
    "    # valid_dl = DataLoader(valid_ds, batch_size=16, shuffle=False, num_workers =8)\n",
    "    train_dl = DataLoader(train_ds, batch_size=256, shuffle=True, num_workers =8)\n",
    "    valid_dl = DataLoader(valid_ds, batch_size=64, shuffle=False, num_workers =8)\n",
    "    return valid_dl,train_dl\n",
    "\n",
    "\n",
    "\n",
    "def getBatchValues(batch, model):\n",
    "    (images,csv_data ) , labels = batch\n",
    "    \n",
    "    #Add variables to GPU device\n",
    "    images = torch.as_tensor(images, device= device, dtype = torch.float32)\n",
    "    csv_data = torch.as_tensor(csv_data, device= device, dtype = torch.float32)\n",
    "    labels = torch.as_tensor(labels, device= device, dtype = torch.float32)\n",
    "    out_probs = model(images, csv_data) \n",
    "    actual_labels = labels.unsqueeze(1).float()\n",
    "    # From log probabilities to actual probabilities to 0 and 1\n",
    "    # Sigmoid Function for binary classification\n",
    "    out_labels = torch.round(torch.sigmoid(out_probs))   \n",
    "    return out_labels, actual_labels, out_probs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def trainData(folds_list, model_name):\n",
    "    for fold, (train_index, test_index) in enumerate(folds_list):\n",
    "        print(f'Fold-{fold +1} ----------------------------------------')\n",
    "        valid_dl,train_dl = getDataLoaoders(train_index, test_index)\n",
    "        model = None\n",
    "        if model_name == \"res\":\n",
    "            model = Resnet50Netwrok().to(device)\n",
    "        else:\n",
    "            model = EfficientNetwork(model_name).to(device)\n",
    "        params = model.parameters()\n",
    "        optimizer = torch.optim.Adam(params, lr =0.001, weight_decay= 0.0)\n",
    "        \n",
    "        # Reduce learning rate when no improvement in performance\n",
    "        # Number of epochs with no improvement after which learning rate will be reduced.\n",
    "        sceduler = ReduceLROnPlateau(optimizer=optimizer, mode=\"max\", patience=1, verbose=True, factor=0.4)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        #Best ROC for the fold\n",
    "        best_roc = None\n",
    "        # If no ROC improvement in 3 epcohs, we stop training there\n",
    "        roc_patience = 3\n",
    "        \n",
    "        for epoch in range(15):\n",
    "            start_time = time.time()\n",
    "            # Model in training mode\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            correct_labels = 0\n",
    "            for k ,batch in enumerate(train_dl):\n",
    "                # Set optimizer gradient to zero atfter each training\n",
    "                optimizer.zero_grad()\n",
    "                out_labels, labels, out_probs = getBatchValues(batch, model)\n",
    "                \n",
    "                loss = criterion(out_probs,labels)\n",
    "                #Backward propgation through model\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss = train_loss + loss.cpu().item()\n",
    "                correct_label_count = (out_labels == labels).sum().item()\n",
    "                correct_labels = correct_labels + correct_label_count\n",
    "                \n",
    "            accuracy = correct_labels/len(train_index)*100\n",
    "            #print(\"Training Accuracy is \", accuracy)\n",
    "            end_time = time.time()\n",
    "            # print(f'Training time for epoch {epoch+1} {(end_time-start_time)/60} mins. Accuracy {accuracy}. train_loss:{train_loss}')\n",
    "            # Start model evaluation\n",
    "            \n",
    "            start_time = time.time()\n",
    "            model.eval()\n",
    "            valid_pred = None\n",
    "            valid_target = None\n",
    "            out_probs_auc = None\n",
    "            valid_roc = 0\n",
    "            # No backward propogation needed since we are wvaluating, so no gradients requires\n",
    "            with torch.no_grad():\n",
    "                for batch in valid_dl:\n",
    "                    valid_roc = 0\n",
    "                    out_labels, labels, out_probs = getBatchValues(batch, model)\n",
    "#                     print('Out train', loss,out_labels.shape,labels.shape)\n",
    "                    if valid_pred is None:\n",
    "                        valid_pred = out_labels\n",
    "                        valid_target = labels\n",
    "                        out_probs_auc = out_probs\n",
    "                    else:\n",
    "                        valid_pred = torch.cat((valid_pred,out_labels), dim=0)\n",
    "                        valid_target = torch.cat((valid_target,labels), dim=0)\n",
    "                        out_probs_auc = torch.cat((out_probs_auc,out_probs), dim=0)\n",
    "                valid_target = valid_target.cpu()\n",
    "                valid_pred = valid_pred.cpu()\n",
    "                valid_acc = accuracy_score(valid_target,valid_pred)\n",
    "                out_probs_auc = torch.sigmoid(out_probs_auc).cpu()\n",
    "                #print('valid Accuracy',valid_acc)\n",
    "#                 try:\n",
    "                valid_roc = roc_auc_score(valid_target,out_probs_auc)\n",
    "                    #print('Valid roc',valid_roc)\n",
    "#                 except:\n",
    "#                     print(\"Cannot find valid ROC\")\n",
    "            end_time = time.time()\n",
    "            #print(f'Valid time for epoch {epoch+1} {(end_time-start_time)/60} mins. Acc: {valid_acc} roc: {valid_roc}')\n",
    "            \n",
    "            if not best_roc:\n",
    "                best_roc = valid_roc\n",
    "                #print('Saving model')\n",
    "                torch.save(model.state_dict(), f\"{model_name}_fo_{fold+1}_ep{epoch+1}_acc_{valid_acc:.3f}_roc_{valid_roc:.3f}.pth\")\n",
    "            elif valid_roc > best_roc:\n",
    "                #print('Saving model Again')\n",
    "                best_roc = valid_roc\n",
    "                # Reset patience becuase model has improvement\n",
    "                roc_patience = 3\n",
    "                torch.save(model.state_dict(), f\"{model_name}_fo_{fold+1}_ep{epoch+1}_acc_{valid_acc:.3f}_roc_{valid_roc:.3f}.pth\")\n",
    "            else:\n",
    "                #Model not improving so reduce number of epochs to consider\n",
    "                roc_patience = roc_patience -1\n",
    "            print(f'Epoch-{epoch+1}| Loss-{train_loss}| Train_acc-{accuracy}| valid_acc-{valid_acc}| ROC: {valid_roc}') \n",
    "            if roc_patience == 0:\n",
    "                print(f'No preformance improving. Early stopping at epoch: {epoch+1},Best ROC: {best_roc}')\n",
    "                break\n",
    "            sceduler.step(valid_roc)\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = np.zeros(len(train))\n",
    "y = train['target']\n",
    "strat_kfold = StratifiedKFold(n_splits=6,shuffle= True)\n",
    "folds = strat_kfold.split(X, y)\n",
    "fold_list = list(folds)\n",
    "trainData(fold_list,\"b4\")\n",
    "\n",
    "# a = train_df[1:1000]\n",
    "# sample_groups = a['patient_id']\n",
    "# X = np.zeros(len(a))\n",
    "# y= a['target']\n",
    "# sample_group_kfold = StratifiedKFold(n_splits=6,shuffle= True)\n",
    "# sample_folds = sample_group_kfold.split(X, y, sample_groups)\n",
    "# # To have multiple iterations\n",
    "# sample_fold_list = list(sample_folds)\n",
    "\n",
    "# # sample_model = EfficientNetwork(\"b2\").to(device)\n",
    "# trainData(sample_fold_list,\"b2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "groups = train['patient_id']\n",
    "X = np.zeros(len(train))\n",
    "y = train['target']\n",
    "group_kfold = GroupKFold(n_splits=6)\n",
    "folds = group_kfold.split(X, y, groups)\n",
    "# To have multiple iterations\n",
    "fold_list = list(folds)\n",
    "trainData(fold_list,\"res\")\n",
    "\n",
    "\n",
    "# a = train[0:1000]\n",
    "# sample_groups = a['patient_id']\n",
    "# X = np.zeros(len(a))\n",
    "# y= a['target']\n",
    "# sample_group_kfold = GroupKFold(n_splits=5)\n",
    "# sample_folds = sample_group_kfold.split(X, y, sample_groups)\n",
    "# # To have multiple iterations\n",
    "# sample_fold_list = list(sample_folds)\n",
    "\n",
    "# # sample_model = Resnet50Netwrok().to(device)\n",
    "# trainData(sample_fold_list,\"res\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# for f in os.listdir():\n",
    "#     os.remove(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path =\"../input/melonomamodel/b2.pth\"\n",
    "b2_model_best = EfficientNetwork(\"b2\").to(device)\n",
    "b2_model_best.load_state_dict(torch.load(model_path))\n",
    "b2_model_best.eval()\n",
    "test_ds= MelanomaDataset(test_df,0.5, 0.5, full_transform = True, isTest= True)\n",
    "test_dl = DataLoader(test_ds, batch_size=16, shuffle=False, num_workers=8)\n",
    "submission = torch.zeros(size = (len(test_df), 1), dtype=torch.float32, device=device)\n",
    "with torch.no_grad():\n",
    "    for i in range(3):\n",
    "        for k,batch in enumerate(test_dl):\n",
    "            images,csv_data = batch\n",
    "            images = torch.as_tensor(images, device= device, dtype = torch.float32)\n",
    "            csv_data = torch.as_tensor(csv_data, device= device, dtype = torch.float32)\n",
    "            out_probs = b2_model_best(images, csv_data)\n",
    "            m = images.shape[0]\n",
    "            probs = torch.sigmoid(out_probs)\n",
    "            submission[k*m: k*m+m] = submission[i*m: i*m+m] + probs\n",
    "submission_df = pd.read_csv('/kaggle/input/siim-isic-melanoma-classification/sample_submission.csv')\n",
    "test_target = submission/3\n",
    "test_target = test_target.cpu().numpy()\n",
    "submission_df['target'] = test_target\n",
    "submission_df.to_csv(f'b2_sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_path =\"../input/melres2/res-2.pth\"\n",
    "res_model_best = Resnet50Netwrok().to(device)\n",
    "res_model_best.load_state_dict(torch.load(model_path))\n",
    "res_model_best.eval()\n",
    "test_ds= MelanomaDataset(test_df,0.5, 0.5, full_transform = False, isTest= True)\n",
    "test_dl = DataLoader(test_ds, batch_size=16, shuffle=False, num_workers=8)\n",
    "submission = torch.zeros(size = (len(test_df), 1), dtype=torch.float32, device=device)\n",
    "with torch.no_grad():\n",
    "    for k,batch in enumerate(test_dl):\n",
    "        images,csv_data = batch\n",
    "        images = torch.as_tensor(images, device= device, dtype = torch.float32)\n",
    "        csv_data = torch.as_tensor(csv_data, device= device, dtype = torch.float32)\n",
    "        out_probs = res_model_best(images, csv_data)\n",
    "        m = images.shape[0]\n",
    "        probs = torch.sigmoid(out_probs)\n",
    "        submission[k*m: k*m+m] =  probs\n",
    "submission_df = pd.read_csv('/kaggle/input/siim-isic-melanoma-classification/sample_submission.csv')\n",
    "test_target = submission.cpu().numpy()\n",
    "submission_df['target'] = test_target\n",
    "submission_df.to_csv('res2_sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path =\"../input/melonomab4/b4.pth\"\n",
    "b4_model_best = EfficientNetwork(\"b4\").to(device)\n",
    "b4_model_best.load_state_dict(torch.load(model_path))\n",
    "b4_model_best.eval()\n",
    "test_ds= MelanomaDataset(test_df,0.5, 0.5, full_transform = False, isTest= True)\n",
    "test_dl = DataLoader(test_ds, batch_size=16, shuffle=False, num_workers=8)\n",
    "submission = torch.zeros(size = (len(test_df), 1), dtype=torch.float32, device=device)\n",
    "with torch.no_grad():\n",
    "    for k,batch in enumerate(test_dl):\n",
    "        images,csv_data = batch\n",
    "        images = torch.as_tensor(images, device= device, dtype = torch.float32)\n",
    "        csv_data = torch.as_tensor(csv_data, device= device, dtype = torch.float32)\n",
    "        out_probs = b4_model_best(images, csv_data)\n",
    "        m = images.shape[0]\n",
    "        probs = torch.sigmoid(out_probs)\n",
    "        submission[k*m: k*m+m] =  probs\n",
    "submission_df = pd.read_csv('/kaggle/input/siim-isic-melanoma-classification/sample_submission.csv')\n",
    "test_target = submission.cpu().numpy()\n",
    "submission_df['target'] = test_target\n",
    "submission_df.to_csv('b4_sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainSplitData(model_name,train_index, test_index):\n",
    "    valid_dl,train_dl = getDataLoaoders(train_index, test_index)\n",
    "    model = None\n",
    "    if model_name == \"res\":\n",
    "        model = Resnet50Netwrok().to(device)\n",
    "    else:\n",
    "        model = EfficientNetwork(model_name).to(device)\n",
    "    params = model.parameters()\n",
    "    optimizer = torch.optim.Adam(params, lr =0.001, weight_decay= 0.0)\n",
    "    sceduler = ReduceLROnPlateau(optimizer=optimizer, mode=\"max\", patience=1, verbose=True, factor=0.4)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    best_roc = None\n",
    "    roc_patience = 8\n",
    "    for epoch in range(15):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct_labels = 0\n",
    "        for k ,batch in enumerate(train_dl):\n",
    "            # Set optimizer gradient to zero atfter each training\n",
    "            optimizer.zero_grad()\n",
    "            out_labels, labels, out_probs = getBatchValues(batch, model)\n",
    "            loss = criterion(out_probs,labels)\n",
    "            #Backward propgation through model\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss = train_loss + loss.cpu().item()\n",
    "            correct_label_count = (out_labels == labels).sum().item()\n",
    "            correct_labels = correct_labels + correct_label_count\n",
    "                \n",
    "        accuracy = correct_labels/len(train_index)*100\n",
    "\n",
    "        model.eval()\n",
    "        valid_pred = None\n",
    "        valid_target = None\n",
    "        out_probs_auc = None\n",
    "        valid_roc = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_dl:\n",
    "                valid_roc = 0\n",
    "                out_labels, labels, out_probs = getBatchValues(batch, model)\n",
    "                if valid_pred is None:\n",
    "                    valid_pred = out_labels\n",
    "                    valid_target = labels\n",
    "                    out_probs_auc = out_probs\n",
    "                else:\n",
    "                    valid_pred = torch.cat((valid_pred,out_labels), dim=0)\n",
    "                    valid_target = torch.cat((valid_target,labels), dim=0)\n",
    "                    out_probs_auc = torch.cat((out_probs_auc,out_probs), dim=0)\n",
    "            valid_target = valid_target.cpu()\n",
    "            valid_pred = valid_pred.cpu()\n",
    "            valid_acc = accuracy_score(valid_target,valid_pred)\n",
    "            out_probs_auc = torch.sigmoid(out_probs_auc).cpu()\n",
    "            valid_roc = roc_auc_score(valid_target,out_probs_auc)            \n",
    "            if not best_roc:\n",
    "                best_roc = valid_roc\n",
    "                #print('Saving model')\n",
    "            elif valid_roc > best_roc:\n",
    "                #print('Saving model Again')\n",
    "                best_roc = valid_roc\n",
    "                # Reset patience becuase model has improvement\n",
    "                roc_patience = 8\n",
    "            else:\n",
    "                #Model not improving so reduce number of epochs to consider\n",
    "                roc_patience = roc_patience -1\n",
    "            torch.save(model.state_dict(), f\"{model_name}_ep{epoch+1}_acc_{valid_acc:.3f}_roc_{valid_roc:.3f}.pth\")\n",
    "        print(f'Epoch-{epoch+1}| Loss-{train_loss}| Train_acc-{accuracy}| valid_acc-{valid_acc}| ROC: {valid_roc}') \n",
    "        if roc_patience == 0:\n",
    "            print(f'No preformance improving. Early stopping at epoch: {epoch+1},Best ROC: {best_roc}')\n",
    "            break\n",
    "        sceduler.step(valid_roc)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns=['target'])\n",
    "y = train['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.16, random_state=42, shuffle = True, stratify = y)\n",
    "trainSplitData('b6',y_train.index, y_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = train[train['target']== 1][0:50]\n",
    "b = train[train['target']== 0][0:100]\n",
    "a= pd.concat([c,b])\n",
    "X = a.drop(columns=['target'])\n",
    "y = a['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.16, random_state=42, shuffle = True, stratify = y)\n",
    "trainSplitData('b4',y_train.index, y_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path =\"./b4_ep14_acc_0.973_roc_0.983.pth\"\n",
    "b4_model = EfficientNetwork(\"b4\").to(device)\n",
    "b4_model.load_state_dict(torch.load(model_path))\n",
    "b4_model.eval()\n",
    "test_ds= MelanomaDataset(test_df,0.5, 0.5, full_transform = False, isTest= True)\n",
    "test_dl = DataLoader(test_ds, batch_size=16, shuffle=False, num_workers=8)\n",
    "submission = torch.zeros(size = (len(test_df), 1), dtype=torch.float32, device=device)\n",
    "with torch.no_grad():\n",
    "    for k,batch in enumerate(test_dl):\n",
    "        images,csv_data = batch\n",
    "        images = torch.as_tensor(images, device= device, dtype = torch.float32)\n",
    "        csv_data = torch.as_tensor(csv_data, device= device, dtype = torch.float32)\n",
    "        out_probs = b4_model(images, csv_data)\n",
    "        m = images.shape[0]\n",
    "        probs = torch.sigmoid(out_probs)\n",
    "        submission[k*m: k*m+m] =  probs\n",
    "submission_df = pd.read_csv('/kaggle/input/siim-isic-melanoma-classification/sample_submission.csv')\n",
    "test_target = submission.cpu().numpy()\n",
    "submission_df['target'] = test_target\n",
    "submission_df.to_csv('b4_str_fold_14.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
